{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up python environment ([do not use pylab](http://carreau.github.io/posts/10-No-PyLab-Thanks.ipynb.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following cost function $ f(X) $ defined\n",
    "over the space of real $ n \\times p $ matrices\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $, and the matrix $ C $ is symmetric, \n",
    "such that $ C = C^T $. The scalar $ \\mu $ is assumed to be larger than the $p$th smallest \n",
    "eigenvalue of $ C $. The matrix $ N $ is diagonal with distinct positive entries\n",
    "on the diagonal.\n",
    "The trace of a square matrix $ A $ is denoted as $ \\trace{A} $, and\n",
    "the Frobenius norm of an arbitrary matrix $ A $ is defined as $ \\Norm{A}_F = \\sqrt{\\trace{A^T A}} $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $ using the formula given. (Use ```numpy.trace``` and ```numpy.sqrt```.)\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```\n",
    "using the formula above?\n",
    "2. Can you come up with a faster implementation, if you were additionally told that $ p \\ge n $ ?\n",
    "3. Can you find an even faster implementation than in 1. and 2.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Given $A \\in \\RR^{n \\times p} $, $ p \\le n $, the straightforward implementation of $ \\trace{A^T A} = \\trace{A A^T}$ is first multiplying the matrices\n",
    "and then taking the trace. This is of\n",
    "complexity $ O(p^2 n) $ (or even $ O(p n^2) $ if you are not careful). \n",
    "But this trace can be reformulated as\n",
    "\\begin{equation}\n",
    "  \\trace{A^T A} = \\sum_{i=1}^p (A^T A)_{i,i} \n",
    "                = \\sum_{i=1}^p \\sum_{j=1}^n \\underbrace{(A^T)_{i,j}}_{=A_{j, i}} A_{j,i}\n",
    "                = \\sum_{i=1}^p \\sum_{j=1}^n A_{j,i}^2\n",
    "\\end{equation}\n",
    "So we can implement it with complexity $ O(np)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15502326  0.13429853  0.74008862]\n",
      " [ 0.95082619  0.29936483  0.97836668]\n",
      " [ 0.99972031  0.59680311  0.36929488]\n",
      " [ 0.76776975  0.21786436  0.73966686]\n",
      " [ 0.47476193  0.28902474  0.40147122]]\n",
      "2.38470954001\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def frobenius_norm(A):\n",
    "    \"\"\"Calculate the Frobenius norm of an array or matrix.\n",
    "    A -- array or matrix\n",
    "    \"\"\"\n",
    "    return np.sqrt((np.asarray(A)**2).sum(axis=None))\n",
    "\n",
    "M = np.random.rand(5,3)\n",
    "print(M)\n",
    "print(frobenius_norm(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with matrix argument\n",
    "\n",
    "Implement the cost function defined as $f(X)$ above as a function ```cost_function_for_matrix```\n",
    "in Python.\n",
    "\n",
    "Hint: As good programmers, we do not use global variables in subroutines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.92791513383\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def cost_function_for_matrix(X, C, N, mu):\n",
    "    \"\"\"\n",
    "    Calculate the cost function at point X given as a matrix.\n",
    "    X -- current point in matrix form\n",
    "    C -- symmetric matrix\n",
    "    N -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    returns the value of the cost function as a scalar.\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.matrix):\n",
    "        raise TypeError(\"X is not a matrix\")\n",
    "\n",
    "    if not isinstance(C, np.matrix):\n",
    "        raise TypeError(\"C is not a matrix\")\n",
    "\n",
    "    if not isinstance(N, np.matrix):\n",
    "        raise TypeError(\"N is not a matrix\")\n",
    "\n",
    "    r1 = 0.5  * np.trace(X.T * C * X * N)\n",
    "    r2 = 0.25 * mu * frobenius_norm(N - X.T * X)**2\n",
    "    return r1 + r2\n",
    "\n",
    "X = np.matrix(np.random.rand(5,3))\n",
    "C = np.random.rand(5,5)\n",
    "C = np.matrix(C+C.T)\n",
    "N = np.matrix(np.diag(np.random.rand(3)))\n",
    "print(cost_function_for_matrix(X,C,N,np.random.rand()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with vector argument\n",
    "\n",
    "Many standard optimisation routines work only with vectors. Fortunately, as vector spaces,\n",
    "the space of matrices $ \\RR^{n \\times p} $ \n",
    "and the space of vectors $ \\RR^{n p} $ are isomorphic. What does this mean?\n",
    "\n",
    "Implement the cost function $ f(X) $ given $ X $ as a vector and call it ```cost_function_for_vector```.\n",
    "Which extra arguments do you need for the cost function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def cost_function_for_vector(X, C, N, mu, n, p):\n",
    "    \"\"\"Calculate the cost function at point X given as 1-D array\n",
    "    X  -- current point as 1-D array\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    n  -- row dimension of X\n",
    "    p  -- column dimension of X\n",
    "    returns the value of the cost function as a scalar\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        raise TypeError(\"X is not a matrix\")\n",
    "\n",
    "    if X.ndim != 1:\n",
    "        raise ValueError(\"X is not a 1-D vector\")\n",
    "\n",
    "    Xmatrix = np.matrix(X.reshape((n, p)))\n",
    "    return cost_function_for_matrix(Xmatrix, C, N, mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "A diagonal matrix has the nice property that the eigenvalues can be directly read off\n",
    "the diagonal. Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues,\n",
    "how many different matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a set of $ n $ distinct real eigenvalues $ \\mathcal{E} = \\{e_1, \\dots, e_n \\} $, \n",
    "write a Python function \\lstinline$random_matrix_from_eigenvalues$ which takes a list of\n",
    "eigenvalues $ E $ and returns a random symmetric matrix $ C $ having the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "There are $ n! $ permutations of diagonal elements, but infinitely many matrices\n",
    "with the same set of eigenvalues.\n",
    "\n",
    "In order to construct a random matrix with given eigenvalues $\\lambda_i$, $i=1,\\dots,n$\n",
    "we first create a diagonal matrix $ \\Lambda $ with those eigenvalues on the\n",
    "diagonal. Then we can get another matrix $ A $ with the same eigenvalues as $ \\Lambda $\n",
    "if we apply an arbitrary nonsingular matrix $ B $ to get $ A = B \\Lambda B^{-1} $.\n",
    "(Can you prove that $ A $ and $ \\Lambda $ have the same set of eigenvalues?)\n",
    "\n",
    "If $ B $ is an orthogonal matrix $ Q $, then $ Q^{-1} = Q^T $ and therefore the above\n",
    "formula results in $ A = Q \\Lambda Q^T $ which is much faster to calculate then\n",
    "using the inverse of a matrix.\n",
    "\n",
    "How to get a random orthogonal matrix? We use the QR-decomposition of a matrix which \n",
    "decomposes every arbitrary matrix $ B $ into an orthogonal matrix $ Q $ and an \n",
    "upper-triangular matrix $ R $, $ B = Q R $.\n",
    "\n",
    "The algorithm therefore is\n",
    "1. Choose a random matrix $ B $ by randomly choosing the elements of $ B $.\n",
    "2. Calculate the QR-decomposition $ B = Q R $. (Check that $ B $ is nonsingular\n",
    "      by checking the diagonal of $ R $ has nonzero elements.)\n",
    "3. Calculate $ A =  Q \\Lambda Q^T $, the wanted arbitrary matrix with the\n",
    "      same eigenvalues as $ \\Lambda $.\n",
    "\n",
    "Note: $ A $ and $ \\Lambda $ share the same \\emph{set} of eigenvalues. The order can\n",
    "be arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def random_matrix_from_eigenvalues(E):\n",
    "    \"\"\"Create a square random matrix with a given set of eigenvalues\n",
    "    E -- list of eigenvalues\n",
    "    return the random matrix\n",
    "    \"\"\"\n",
    "    n    = len(E)\n",
    "    # Create a random orthogonal matrix Q via QR decomposition\n",
    "    # of a random matrix A\n",
    "    A    = np.matrix(np.random.rand(n,n))\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    #  similarity transformation with orthogonal\n",
    "    #  matrix leaves eigenvalues intact\n",
    "    return Q * np.diag(E) * Q.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$\n",
    "\n",
    "Use the minimisation functions ```fmin``` or ```fmin_powell``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector```.\n",
    "\n",
    "Hint: Use the argument ```args``` in the minimisation functions  ```fmin``` or ```fmin_powell``` \n",
    "to provide the extra parameters to\n",
    "your cost function ```cost_function_for_vector```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def minimise_f_using_fmin(initialise_proc):\n",
    "    \"\"\"Run minimisation with simplex algorithm.\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 xtol=0.0001,\n",
    "                                 ftol=0.0001,\n",
    "                                 maxiter=None,\n",
    "                                 maxfun=None,\n",
    "                                 full_output = 0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n",
    "\n",
    "def minimise_f_using_fmin_powell(initialise_proc):\n",
    "    \"\"\"Run minimisation with Powell algorithm\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin_powell(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 xtol=0.0001,\n",
    "                                 ftol=0.0001,\n",
    "                                 maxiter=None,\n",
    "                                 maxfun=None,\n",
    "                                 full_output = 0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None,\n",
    "                                 direc=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of $f(X)$\n",
    "\n",
    "Calculate the gradient for the cost function $f(X)$ given the\n",
    "inner product on the space of real matrices $ n \\times p $ is defined as\n",
    "\\begin{equation}\n",
    "  \\inner{A}{B} = \\trace{A^T B}\n",
    "\\end{equation}\n",
    "\n",
    "Implement a function ```gradient_for_vector``` which takes $ X $ as a vector, and\n",
    "returns the gradient of $ f(X) $ as a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The definition of the directional derivative is in the slides.\n",
    "A straightforward calculation using the definition gives\n",
    "the directional derivative of the cost function as\n",
    "\n",
    "\\begin{align*}\n",
    "  \\DD f(X) (\\xi) = & \\phantom{+} \\frac{1}{2} \\trace{\\xi^T C X N} \\\\\n",
    "                   & + \\frac{1}{2} \\trace{X^T C \\xi N} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(- \\xi^T X)(N - X^T X)} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(- X^T \\xi)(N - X^T X)} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(N - X^T X)(- \\xi^T X)} \\\\\n",
    "                   & + \\frac{1}{4} \\mu \\trace{(N - X^T X)(- X^T \\xi)} .               \n",
    "\\end{align*}\n",
    "\n",
    "Note, the shortcut was to replace each occurrence of the variable $ X $\n",
    "in the function $ f(X) $ once with $ \\xi $ and then add together \n",
    "all those expressions. Reason: The directional derivative gives the\n",
    "linear approximation of the infinitesimal change of the function $ f(X) $\n",
    "at $ X $ in direction $ \\xi $. Therefore it must be linear in $ \\xi $.\n",
    "\n",
    "The above expression can be simplified by using that for any matrices\n",
    "$ A, B $,\n",
    "\n",
    "\\begin{align*}\n",
    " \\trace{A^T} & = \\trace{A}, \\\\\n",
    " \\trace{A B} & = \\trace{B A}\n",
    "\\end{align*}\n",
    " \n",
    "From this we get therefore the simplified form\n",
    "\\begin{align*}\n",
    "  \\DD f(X) (\\xi) & = \\trace{\\xi^T C X N} - \\mu \\trace{\\xi^T X(N - X^T X)} \\\\\n",
    "                 & = \\trace{\\xi^T \\left(C X N - \\mu X(N - X^T X) \\right)}\n",
    "\\end{align*}\n",
    "\n",
    "For the given inner product $ \\inner{A}{B} = \\trace{A^T B} $, the gradient\n",
    "will therefore be\n",
    "\n",
    "\\begin{equation}\n",
    "   \\grad f(X) = C X N - \\mu X(N - X^T X)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def gradient_for_matrix(X, C, N, mu):\n",
    "    \"\"\"Calculate the gradient for the cost function at a point X\n",
    "    X  -- current point in matrix form\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    returns the gradient of the cost function as matrix\n",
    "    \"\"\"\n",
    "    gradient = C * X * N - mu * X * (N - X.T * X)\n",
    "    return gradient\n",
    "\n",
    "def gradient_for_vector(X, C, N, mu, n, p):\n",
    "    \"\"\"Calculate the gradient for the cost function at a point X\n",
    "    X  -- current point as 1-D array\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    n  -- row dimension of X\n",
    "    p  -- column dimension of X\n",
    "    returns the gradient of the cost function as 1-D array\n",
    "    \"\"\"\n",
    "    Xmatrix = np.matrix(X.reshape((n, p)))\n",
    "    gradient =  gradient_for_matrix(Xmatrix, C, N, mu)\n",
    "    return np.asarray(gradient).reshape((n*p,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$ using the gradient\n",
    "\n",
    "Use the minimisation functions ```fmin_cg``` or ```fmin_bfgs``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector``` utilising the gradient ```gradient_for_vector```.\n",
    "\n",
    "Compare the speed of convergence to the minimisation with ```fmin``` or ```fmin_powell```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def normalize_columns(A):\n",
    "    \"\"\"Normalise the columns of a 2-D array or matrix to length one\n",
    "    A - array or matrix (which will be modified)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"A is not a 2-D array\")\n",
    "\n",
    "    number_of_columns = A.shape[1]\n",
    "    for i in range(number_of_columns):\n",
    "        A[:,i] /= np.linalg.norm(A[:,i], ord=2)\n",
    "\n",
    "\n",
    "def show_results(X_at_min, C):\n",
    "    \"\"\"Display the found arg min and compare with eigenvalues of C\n",
    "    X_at_min -- arguement at minimum found\n",
    "    C        -- symmetric matrix\n",
    "    \"\"\"\n",
    "    n,p = X_at_min.shape\n",
    "\n",
    "    normalize_columns(X_at_min)\n",
    "\n",
    "    # Get the eigenvectors belonging to the smallest eigenvalues\n",
    "    Eigen_Values, Eigen_Vectors = np.linalg.eig(C)\n",
    "    Permutation = Eigen_Values.argsort()\n",
    "    Smallest_Eigenvectors = Eigen_Vectors[:, Permutation[:p]]\n",
    "\n",
    "    if n < 10:\n",
    "        print(\"X_at_min               :\\n\", X_at_min)\n",
    "        print()\n",
    "        print(\"Smallest_Eigenvectors  :\\n\", Smallest_Eigenvectors)\n",
    "        print()\n",
    "    else:\n",
    "        Project_into_Eigenvectorspace = \\\n",
    "          Smallest_Eigenvectors * Smallest_Eigenvectors.T * X_at_min\n",
    "        Normal_Component = X_at_min - Project_into_Eigenvectorspace\n",
    "\n",
    "        print(\"norm(Normal_Component)/per entry :\", \\\n",
    "            np.linalg.norm(Normal_Component, ord=2) / float(n*p))\n",
    "\n",
    "\n",
    "\n",
    "def minimise_f_using_fmin_cg(initialise_proc):\n",
    "    \"\"\"Run minimisation with conjugate gradient algorithm\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin_cg(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 fprime=gradient_for_vector,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 gtol=1.0000000000000001e-05,\n",
    "                                 norm=2,\n",
    "                                 epsilon=1.49011611938e-08,\n",
    "                                 maxiter=None,\n",
    "                                 full_output=0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n",
    "\n",
    "\n",
    "def minimise_f_using_fmin_bfgs(initialise_proc):\n",
    "    \"\"\"Run minimisation with BFGS algorithm\"\"\"\n",
    "    C, N, mu, n, p, X0 = initialise_proc()\n",
    "\n",
    "    X_at_min = opt.fmin_bfgs(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 fprime=gradient_for_vector,\n",
    "                                 args=(C, N, mu, n, p),\n",
    "                                 gtol=1.0000000000000001e-05,\n",
    "                                 norm=2,\n",
    "                                 epsilon=1.49011611938e-08,\n",
    "                                 maxiter=None,\n",
    "                                 full_output=0,\n",
    "                                 disp=1,\n",
    "                                 retall=0,\n",
    "                                 callback=None)\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    show_results(X_at_min, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minima of $f(X)$\n",
    "\n",
    "Compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The minimum of the given cost function are matrices $ X $ which contain\n",
    "the $ p $ eigenvectors of $ C $ which are associated with the $ p $ smallest\n",
    "eigenvalues of $ C $. The order of the eigenvector in the minimum $ X $\n",
    "is defined by the order of the diagonal elements in $ N $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "=========  minimise_f_using_fmin(initialise_low_dimensional_data)  =========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 374\n",
      "         Function evaluations: 581\n",
      "X_at_min               :\n",
      " [[ 0.59071163 -0.25311288]\n",
      " [ 0.44344838 -0.6914357 ]\n",
      " [ 0.67410185  0.67664654]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[ 0.59074381 -0.25320764]\n",
      " [ 0.44342824 -0.69135772]\n",
      " [ 0.6740869   0.67669077]]\n",
      "\n",
      "run_time : 0.04451500000000008\n",
      "============================================================================\n",
      "=======  minimise_f_using_fmin(initialise_higher_dimensional_data)  ========\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def initialise_low_dimensional_data():\n",
    "    \"\"\"Initialise the data, low dimensions\"\"\"\n",
    "    n = 3\n",
    "    p = 2\n",
    "    mu = 2.7\n",
    "\n",
    "    N = np.matrix(np.diag([2.5, 1.5]))\n",
    "    E = [1, 2, 3]\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def initialise_higher_dimensional_data():\n",
    "    \"\"\"Initialise the data, higher dimensions\"\"\"\n",
    "    n  = 20\n",
    "    p  =  5\n",
    "    mu = p + 0.5\n",
    "\n",
    "    N = np.matrix(np.diag(np.arange(p, 0, -1)))\n",
    "    E = np.arange(1, n+1)\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def run_and_time_all_tests():\n",
    "    \"\"\"Run all test and time them using a list of function names\"\"\"\n",
    "    List_of_Test_Names = [\"minimise_f_using_fmin\",\n",
    "                 \"minimise_f_using_fmin_powell\",\n",
    "                 \"minimise_f_using_fmin_cg\",\n",
    "                 \"minimise_f_using_fmin_bfgs\"]\n",
    "\n",
    "    List_of_Initialisations = [\"initialise_low_dimensional_data\",\n",
    "                               \"initialise_higher_dimensional_data\"]\n",
    "\n",
    "    for test_name in List_of_Test_Names:\n",
    "        for init_routine in List_of_Initialisations:\n",
    "            task_string  = test_name + \"(\" + init_routine + \")\"\n",
    "            line_length  = 76\n",
    "            spaces       = 2\n",
    "            left_padding = (line_length - len(task_string)) // 2\n",
    "            right_padding = line_length - left_padding - len(task_string)\n",
    "            print(\"=\" * line_length)\n",
    "            print(\"=\" * (left_padding - spaces) + \" \" * spaces + task_string + \\\n",
    "                \" \" * spaces + \"=\" * (right_padding - spaces))\n",
    "            print(\"=\" * line_length)\n",
    "\n",
    "            start = time.clock()\n",
    "            exec(task_string)\n",
    "            run_time = time.clock() - start\n",
    "            print(\"run_time :\", run_time)\n",
    "\n",
    "run_and_time_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "norm(Normal_Component)/per entry : 0.00753526909048\n",
      "run_time : 2.125085\n",
      "============================================================================\n",
      "=====  minimise_f_using_fmin_powell(initialise_low_dimensional_data)  ======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962965\n",
      "         Iterations: 5\n",
      "         Function evaluations: 361\n",
      "X_at_min               :\n",
      " [[ 0.77498661 -0.27509769]\n",
      " [ 0.08634706  0.93811832]\n",
      " [ 0.62605107  0.2103694 ]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[-0.77515459 -0.27566473]\n",
      " [-0.08607896  0.93747641]\n",
      " [-0.62588     0.21247809]]\n",
      "\n",
      "run_time : 0.019376999999999978\n",
      "============================================================================\n",
      "====  minimise_f_using_fmin_powell(initialise_higher_dimensional_data)  ====\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 40.733405\n",
      "         Iterations: 19\n",
      "         Function evaluations: 20132\n",
      "norm(Normal_Component)/per entry : 0.000264402801202\n",
      "run_time : 1.1543\n",
      "============================================================================\n",
      "=======  minimise_f_using_fmin_cg(initialise_low_dimensional_data)  ========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 18\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 36\n",
      "X_at_min               :\n",
      " [[ 0.37258007  0.52231398]\n",
      " [ 0.73516575  0.33829126]\n",
      " [ 0.56631741 -0.78278166]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[ 0.37258042  0.52231338]\n",
      " [ 0.73516594  0.33829002]\n",
      " [ 0.56631693 -0.7827826 ]]\n",
      "\n",
      "run_time : 0.005345000000000155\n",
      "============================================================================\n",
      "======  minimise_f_using_fmin_cg(initialise_higher_dimensional_data)  ======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 40.727273\n",
      "         Iterations: 109\n",
      "         Function evaluations: 172\n",
      "         Gradient evaluations: 172\n",
      "norm(Normal_Component)/per entry : 6.98639597753e-09\n",
      "run_time : 0.020500000000000185\n",
      "============================================================================\n",
      "======  minimise_f_using_fmin_bfgs(initialise_low_dimensional_data)  =======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 13\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n",
      "X_at_min               :\n",
      " [[ 0.67688694  0.67393957]\n",
      " [ 0.69239463 -0.71944363]\n",
      " [ 0.24982743  0.16794736]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[ 0.67688677  0.67393967]\n",
      " [ 0.69239489 -0.71944333]\n",
      " [ 0.24982718  0.16794825]]\n",
      "\n",
      "run_time : 0.0033969999999996503\n",
      "============================================================================\n",
      "=====  minimise_f_using_fmin_bfgs(initialise_higher_dimensional_data)  =====\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 40.727273\n",
      "         Iterations: 146\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 154\n",
      "norm(Normal_Component)/per entry : 2.1992623561e-08\n",
      "run_time : 0.05705600000000022\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
