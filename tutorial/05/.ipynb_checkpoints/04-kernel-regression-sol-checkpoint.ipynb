{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Get into groups of two or three and take turns explaining the following (about 2 minutes each):\n",
    "- regression vs classification\n",
    "- Fisher's discriminant\n",
    "- generative vs discriminative probabilistic methods\n",
    "- logistic regression\n",
    "- support vector machines\n",
    "- basis functions vs kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\dotprod}[2]{\\langle #1, #2 \\rangle}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "This is the same dataset we used in Tutorial 2.\n",
    "\n",
    "*We will use an old dataset on the price of housing in Boston (see [description](https://archive.ics.uci.edu/ml/datasets/Housing)). The aim is to predict the median value of the owner occupied homes from various other factors. We will use a normalised version of this data, where each row is an example. The median value of homes is given in the first column (the label) and the value of each subsequent feature has been normalised to be in the range $[-1,1]$. Download this dataset from [mldata.org](http://mldata.org/repository/data/download/csv/housing_scale/).*\n",
    "\n",
    "Read in the data using pandas. Remove the column containing the binary variable 'CHAS' using ```drop```, which should give you a DataFrame with 506 rows (examples) and 13 columns (1 label and 12 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
    "data = pd.read_csv('housing_scale.csv', header=None, names=names)\n",
    "data.head()\n",
    "data.drop('chas', axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing new kernels\n",
    "\n",
    "In the lectures, we saw that certain operations on kernels preserve positive semidefiniteness. Recall that a symmetric matrix $K\\in \\RR^n \\times\\RR^n$ is positive semidefinite if for all vectors $a\\in\\RR^n$ we have the inequality\n",
    "$$\n",
    "a^T K a \\geqslant 0.\n",
    "$$\n",
    "\n",
    "Prove the following relations:\n",
    "1. Given positive semidefinite matrices $K_1$, $K_2$, show that $K_1 + K_2$ is a valid kernel.\n",
    "2. Given a positive semidefinite matrix $K$, show that $K^2 = K\\cdot K$ is a valid kernel, where the multiplication is a pointwise multiplication (not matrix multiplication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. We want to prove that\n",
    "$$a^T (K_1 + K_2) a \\geqslant 0.$$\n",
    "By linearity of addition, distribute the multiplication of $a$\n",
    "$$a^T (K_1 + K_2) a = a^T K_1 a^ + a^T K_2 a.$$\n",
    "By the definition of kernels, $a^T K_1 a \\geqslant 0$ and $a^T K_2 a \\geqslant 0$ for all $a$. \n",
    "Since the sum of two non-negative numbers is non-negative, we have shown that $K_1+K_2$ is a valid kernel.\n",
    "\n",
    "2. Consider the eigenvalue decomposition of $K = U^T \\Lambda U$. Recall that a positive semidefinite matrix has non-negative eigenvalues, i.e. $\\Lambda$ is a diagonal matrix and the elements are all non-negative.\n",
    "Then\n",
    "$$K^2 = (U^2)^T \\Lambda^2 U^2.$$\n",
    "Since $\\Lambda$ is a diagonal matrix, its square is just the square of each diagonal element.\n",
    "Since each eigenvalue of $K^2$ is non-negative, then $K^2$ is positive semidefinite. For alternative proofs see\n",
    "[Schur Product Theorem](http://en.wikipedia.org/wiki/Schur_product_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernel using closure\n",
    "\n",
    "Using the properties proven above, show that the inhomogenous polynomial kernel of degree 2\n",
    "$$k(x,y) = (\\dotprod{x}{y} + 1)^2$$\n",
    "is positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Consider the dot product $\\dotprod{x}{y}$ as the linear kernel.\n",
    "\\begin{align}\n",
    "k(x,y) &= (\\dotprod{x}{y} + 1)^2\\\\\n",
    "    &= (\\dotprod{x}{y} + 1)(\\dotprod{x}{y} + 1)\\\\\n",
    "    &= \\dotprod{x}{y}^2 + 2\\dotprod{x}{y} + 1.\n",
    "\\end{align}\n",
    "Since each term above is positive semidefinite, the sum is positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical comparison\n",
    "\n",
    "Recall from Tutorial 2 that we could explicitly construct the polynomial basis function. In fact this demonstrates the relation\n",
    "$$\n",
    "k(x,y) = (\\dotprod{x}{y} + 1)^2 = \\dotprod{\\phi(x)}{\\phi(y)}.\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\phi(x) = (x_1^2, x_2^2, \\ldots, x_n^2, \\sqrt{2}x_1 x_2, \\ldots, \\sqrt{2}x_{n-1} x_n, \\sqrt{2}x_1, \\ldots, \\sqrt{2}x_n, 1)\n",
    "$$\n",
    "*This is sometimes referred to as an explicit feature map or the primal version of a kernel method.*\n",
    "\n",
    "For the data above, construct two kernel matrices, one using the explicit feature map and the second using the equation for the polynomial kernel. Confirm that they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 46.3320328   41.68002662  43.6717265   44.58601525  44.05655316]\n",
      " [ 41.68002662  48.04568901  47.58892083  47.79575945  48.29439918]\n",
      " [ 43.6717265   47.58892083  51.34484729  53.17099659  52.47069978]\n",
      " [ 44.58601525  47.79575945  53.17099659  60.00986675  58.24709642]\n",
      " [ 44.05655316  48.29439918  52.47069978  58.24709642  57.27580037]]\n",
      "[[ 44.92509717  40.21901285  42.27663736  43.47887681  42.85634363]\n",
      " [ 40.21901285  46.68355971  46.2751848   46.61879612  47.01972063]\n",
      " [ 42.27663736  46.2751848   50.05018765  51.94600567  51.1452507 ]\n",
      " [ 43.47887681  46.61879612  51.94600567  58.84464233  57.02168013]\n",
      " [ 42.85634363  47.01972063  51.1452507   57.02168013  55.9939958 ]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def phi_quadratic(x):\n",
    "    \"\"\"Compute phi(x) for a single training example using quadratic basis function.\"\"\"\n",
    "    D = len(x)\n",
    "    # Features are (1, {x_i}, {cross terms and squared terms})\n",
    "    # Cross terms x_i x_j and squared terms x_i^2 can be taken from upper triangle of outer product of x with itself\n",
    "    return np.hstack((1, x, np.sqrt(2)*np.outer(x, x)[np.triu_indices(D)]))\n",
    "\n",
    "def feature_map(X):\n",
    "    \"\"\"Return the matrix of the feature map.\"\"\"\n",
    "    num_ex, num_feat = X.shape\n",
    "    Phi = np.zeros((num_ex, int(1+num_feat+num_feat*(num_feat+1)/2)))\n",
    "    for ix in range(num_ex):\n",
    "        Phi[ix,:] = phi_quadratic(X[ix,:])\n",
    "    return Phi\n",
    "\n",
    "def dotprod_quadratic(X):\n",
    "    \"\"\"Compute the kernel matrix using an explicit feature map of\n",
    "    the inhomogeneous polynomial kernel of degree 2\"\"\"\n",
    "    Phi = feature_map(X)\n",
    "    return np.dot(Phi, Phi.T)\n",
    "\n",
    "def kernel_quadratic(X,Y):\n",
    "    \"\"\"Compute the inhomogenous polynomial kernel matrix of degree 2\"\"\"\n",
    "    lin_dot = np.dot(X,Y.T)\n",
    "    dotprod = (lin_dot+1.)*(lin_dot + 1.)\n",
    "    return dotprod\n",
    "\n",
    "headers = list(data.columns.values)\n",
    "headers.remove('medv')\n",
    "X = np.array(data[headers])\n",
    "K = kernel_quadratic(X,X)\n",
    "Kfeat = dotprod_quadratic(X)\n",
    "\n",
    "print(K[:5,:5])\n",
    "print(Kfeat[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are pros and cons for each method of computing the kernel matrix. Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* The kernel approach is computationally independent of the number of features and the number degree of the polynomial. So if there are many features or a high degree polynomial is required, then this is the computationally cheaper approach.\n",
    "* The feature map approach is computationally independent of the number of examples. If there are many examples $N$, then storing the kernel matrix is quadratic $\\mathcal{O}(N^2)$, but the feature map only needs you to store $\\mathcal{O}(ND^2)$ where D is the number of dimensions.\n",
    "* The feature map approach allows linear methods to be used. The kernel approach requires solving potentially more complex optimization problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized least squares with kernels\n",
    "\n",
    "This section is analogous to the part in Tutorial 2 about regularized least squares.\n",
    "\n",
    "State carefully the cost function and the regulariser, defining all symbols, show that the regularized least squares solution can be expressed as in Lecture 5 and Lecture 9.\n",
    "$$\n",
    "w = \\left( \\lambda \\mathbf{I} + \\Phi^T \\Phi\\right)^{-1} \\Phi t\n",
    "$$\n",
    "Please describe the reason for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "See Lecture 9. The important part is knowing the reason for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By substituting $w = \\Phi^T a$, derive the regularized least squares method in terms of the kernel matrix $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "See Lecture 9. The important part is knowing the reason for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing solutions in $a$ and $\\mathbf{w}$\n",
    "\n",
    "Implement the kernelized regularized least squares as above. \n",
    "*This is often referred to as the dual version of the kernel method.*\n",
    "\n",
    "Compare this with the solution from Tutorial 2. Implement two classes:\n",
    "* ```RLSPrimal```\n",
    "* ```RLSDual```\n",
    "\n",
    "each which contain a ```train``` and ```predict``` function.\n",
    "\n",
    "Think carefully about the interfaces to the training and test procedures for the two different versions of regularized least squares. Also think about the parameters that need to be stored in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: primal = 5.260380, dual = 978.326778\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# Note that the solution below has several programming bugs which need to be fixed before it will work.\n",
    "\n",
    "def feature_map(X):\n",
    "    return X\n",
    "\n",
    "class RLSPrimal(object):\n",
    "    \"\"\"Primal Regularized Least Squares\"\"\"\n",
    "    def __init__(self, reg_param):\n",
    "        self.reg_param = reg_param\n",
    "        self.w = np.array([])   # This should be the number of features long\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Find the maximum likelihood parameters for the data X and labels y\"\"\"\n",
    "        Phi = feature_map(X)\n",
    "        num_ex = (Phi.T).shape[0]\n",
    "        A = np.dot(Phi.T, Phi) + self.reg_param * np.eye(num_ex)\n",
    "        b = np.dot(Phi.T, y)\n",
    "        self.w = np.linalg.solve(A, b)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Assume trained. Predict on data X.\"\"\"\n",
    "        Phi = feature_map(X)\n",
    "        return np.dot(Phi, self.w)\n",
    "\n",
    "class RLSDual(object):\n",
    "    def __init__(self, reg_param):\n",
    "        self.reg_param = reg_param\n",
    "        self.a = np.array([])    # This should be number of examples long\n",
    "\n",
    "    def train(self, K, y):\n",
    "        \"\"\"Find the maximum likelihood parameters for the kernel matrix K and labels y\"\"\"\n",
    "        num_ex = K.shape[0]\n",
    "        A = K + self.reg_param * np.eye(num_ex)\n",
    "        self.a = np.linalg.solve(A, y)\n",
    "    \n",
    "    def predict(self, K):\n",
    "        \"\"\"Assume trained. Predict on test kernel matrix K.\"\"\"\n",
    "        return np.dot(K, self.a)\n",
    "    \n",
    "def split_data(data):\n",
    "    \"\"\"Randomly split data into two equal groups\"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(N/2)]\n",
    "    test_idx = idx[int(N/2):]\n",
    "\n",
    "    X_train = data.loc[train_idx].drop('medv', axis=1)\n",
    "    t_train = data.loc[train_idx]['medv']\n",
    "    X_test = data.loc[test_idx].drop('medv', axis=1)\n",
    "    t_test = data.loc[test_idx]['medv']\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def rmse(label, prediction):\n",
    "    N = len(label)\n",
    "    return np.linalg.norm(label - prediction) / np.sqrt(N)\n",
    "\n",
    "X_train, t_train, X_test, t_test = split_data(data)\n",
    "P = RLSPrimal(1.1)\n",
    "P.train(X_train, t_train)\n",
    "pP = P.predict(X_test)\n",
    "\n",
    "K_train = kernel_quadratic(X_train, X_train)\n",
    "K_test = kernel_quadratic(X_train, X_test)   # This is not square\n",
    "D = RLSDual(1.1)\n",
    "D.train(K_train, t_train)\n",
    "pD = D.predict(K_test)\n",
    "\n",
    "print('RMSE: primal = %f, dual = %f' % (rmse(t_test, pP), rmse(t_test, pD)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) General kernel\n",
    "\n",
    "Consider how you would generalise the two classes above if you wanted to have a polynomial kernel of degree 3. For the primal version, assume you have a function that returns the explicit feature map for the kernel ```feature_map(X)``` and for the dual version assume you have a function that returns the kernel matrix ```kernel_matrix(X)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
